{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status : *COMPLETED*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Instructions:\n",
    "- Import the dataset into a pandas dataframe using the read_table method. \n",
    "- Because this is a tab separated dataset we will be using '\\t' as the value for the 'sep' argument which specifies this format.\n",
    "- Also, rename the column names by specifying a list ['label, 'sms_message'] to the 'names' argument of read_table().\n",
    "- Print the first five values of the dataframe with the new column names.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        sms_message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localpath = '..\\data\\SMSSpamCollection'\n",
    "colnames = ['label', 'sms_message']\n",
    "df = pd.read_table(localpath, sep='\\t', names=colnames)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ">Instructions:\n",
    "- Convert the values in the 'label' colum to numerical values using map method as follows: {'ham':0, 'spam':1} This maps the 'ham' value to 0 and the 'spam' value to 1.\n",
    "- Also, to get an idea of the size of the dataset we are dealing with, print out number of rows and columns using 'shape'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        sms_message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df['label'].map({'ham':0, 'spam':1})\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">```python\n",
    "documents = ['Hello, how are you!',\n",
    "             'Win money, win from home.',\n",
    "             'Call me now.',\n",
    "             'Hello, Call hello you tomorrow?']\n",
    "```  \n",
    "\n",
    ">**Instructions**:\n",
    "Convert all the strings in the documents set to their lower case. Save them into a list called 'lower_case_documents'. You can convert strings to their lower case in python by using the `lower()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> documents = ['Hello, how are you!',\n",
    "                 'Win money, win from home.',\n",
    "                 'Call me now.',\n",
    "                 'Hello, Call hello you tomorrow?']\n",
    "lower_case_documents = []\n",
    "\n",
    "for doc in documents:\n",
    "    item = doc.lower()\n",
    "    lower_case_documents.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?']\n"
     ]
    }
   ],
   "source": [
    "print(lower_case_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Instructions**: Remove all punctuation from the strings in the document set. Save them into a list called 'sans_punctuation_documents'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello how are you', 'win money win from home', 'call me now', 'hello call hello you tomorrow']\n"
     ]
    }
   ],
   "source": [
    "import string as s\n",
    "\n",
    "sans_puntucation_documents = []\n",
    "\n",
    "for doc in lower_case_documents:\n",
    "    for char in s.punctuation:\n",
    "        if char in doc:\n",
    "            doc = doc.replace(char, '')\n",
    "    sans_puntucation_documents.append(doc)\n",
    "print(sans_puntucation_documents)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Udacity solution**\n",
    "\n",
    ">```python\n",
    "sans_punctuation_documents = []\n",
    "import string\n",
    "for i in lower_case_documents:\n",
    "    sans_punctuation_documents.append(\n",
    "        i.translate(\n",
    "            str.maketrans('', '', string.punctuation)\n",
    "            )\n",
    "    )\n",
    "print(sans_punctuation_documents)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following makes a table indicating that *nothing* should be translated to *nothing* while removing `string.punctuation`:\n",
    "```python\n",
    "str.maketrans('', '', string.punctuation)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Instructions: Tokenize the strings stored in 'sans_punctuation_documents' using the split() method. and store the final document set in a list called 'preprocessed_documents'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello how are you', 'win money win from home', 'call me now', 'hello call hello you tomorrow']\n"
     ]
    }
   ],
   "source": [
    "print(sans_punctuation_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'how', 'are', 'you', 'win', 'money', 'from', 'home', 'call', 'me', 'now', 'tomorrow']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_documents = []\n",
    "\n",
    "for doc in sans_punctuation_documents:\n",
    "    docList = doc.split()\n",
    "    for t in docList:\n",
    "        if t not in preprocessed_documents:\n",
    "            preprocessed_documents.append(t)\n",
    "\n",
    "print(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is effective, but not what is the desired output.\n",
    "\n",
    "The desired output is a series of lists of tokens for each entry/doc. Revision below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 'how', 'are', 'you'], ['win', 'money', 'win', 'from', 'home'], ['call', 'me', 'now'], ['hello', 'call', 'hello', 'you', 'tomorrow']]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_documents = []\n",
    "\n",
    "for doc in sans_punctuation_documents:\n",
    "    docList = doc.split()\n",
    "    preprocessed_documents.append(docList)\n",
    "print(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Instructions:**\n",
    "Using the Counter() method and preprocessed_documents as the input, create a dictionary with the keys being each word in each document and the corresponding values being the frequncy of occurrence of that word. Save each Counter dictionary as an item in a list called 'frequency_list'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'how': 1, 'you': 1, 'are': 1, 'hello': 1}), Counter({'win': 2, 'from': 1, 'money': 1, 'home': 1}), Counter({'call': 1, 'now': 1, 'me': 1}), Counter({'hello': 2, 'call': 1, 'you': 1, 'tomorrow': 1})]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "frequency_list = []\n",
    "for doc in preprocessed_documents:\n",
    "    frequency_list.append(Counter(doc))\n",
    "    \n",
    "    # more soph what I originally did\n",
    "    # frequency_list.append(dict(Counter(doc)))\n",
    "print(frequency_list) \n",
    "# for pretty printing, use 'pprint' module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Instructions:**\n",
    "Import the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> documents = ['Hello, how are you!',\n",
    "                'Win money, win from home.',\n",
    "                'Call me now.',\n",
    "                'Hello, Call hello you tomorrow?']\n",
    "\n",
    "from sklearn import feature_extraction\n",
    "# count_vector = feature_extraction.text.CountVectorizer(input=documents)\n",
    "# no need to use the fxn, just need to create an instance\n",
    "count_vector = feature_extraction.text.CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively:\n",
    ">```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "```\n",
    "\n",
    "This is probably better/simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "print(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are',\n",
       " 'call',\n",
       " 'from',\n",
       " 'hello',\n",
       " 'home',\n",
       " 'how',\n",
       " 'me',\n",
       " 'money',\n",
       " 'now',\n",
       " 'tomorrow',\n",
       " 'win',\n",
       " 'you']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.fit(documents)\n",
    "count_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**\n",
    "Instructions:**\n",
    "Create a matrix with the rows being each of the 4 documents, and the columns being each word. \n",
    "The corresponding (row, column) value is the frequency of occurrance of that word(in the column) in a particular\n",
    "document(in the row). You can do this using the transform() method and passing in the document data set as the \n",
    "argument. The transform() method returns a matrix of numpy integers, you can convert this to an array using\n",
    "toarray(). Call the array 'doc_array'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "doc_array = count_vector.transform(documents).toarray()\n",
    "# transform, the CountVectorizer method, not the DF method in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 11)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 10)\t2\n",
      "  (2, 1)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 8)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 3)\t2\n",
      "  (3, 9)\t1\n",
      "  (3, 11)\t1\n",
      "[[1 0 0 1 0 1 0 0 0 0 0 1]\n",
      " [0 0 1 0 1 0 0 1 0 0 2 0]\n",
      " [0 1 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 1 0 2 0 0 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(doc_array) # indicates location of nonzero values (only) and the value itself\n",
    "print(doc_array.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Instructions:**\n",
    "Convert the array we obtained, loaded into 'doc_array', into a dataframe and set the column names to \n",
    "the word names(which you computed earlier using get_feature_names(). Call the dataframe 'frequency_matrix'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>call</th>\n",
       "      <th>from</th>\n",
       "      <th>hello</th>\n",
       "      <th>home</th>\n",
       "      <th>how</th>\n",
       "      <th>me</th>\n",
       "      <th>money</th>\n",
       "      <th>now</th>\n",
       "      <th>tomorrow</th>\n",
       "      <th>win</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n",
       "0    1     0     0      1     0    1   0      0    0         0    0    1\n",
       "1    0     0     1      0     1    0   0      1    0         0    2    0\n",
       "2    0     1     0      0     0    0   1      0    1         0    0    0\n",
       "3    0     1     0      2     0    0   0      0    0         1    0    1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_matrix = pd.DataFrame(data=doc_array, \n",
    "                                columns=count_vector.get_feature_names())\n",
    "frequency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    ">**Instructions:**\n",
    "Split the dataset into a training and testing set by using the train_test_split method in sklearn. Split the data\n",
    "using the following variables:\n",
    "* `X_train` is our training data for the 'sms_message' column.\n",
    "* `y_train` is our training data for the 'label' column\n",
    "* `X_test` is our testing data for the 'sms_message' column.\n",
    "* `y_test` is our testing data for the 'label' column\n",
    "Print out the number of rows we have in each our training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179 1393 \n",
      " 4179 1393\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['sms_message']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(len(X_train),len(X_test),'\\n',len(y_train),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiation\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# fitting data + returning mtx simult.\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "# transform testing data + returning mtx\n",
    "testing_data = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2022)\t1\n",
      "  (0, 4779)\t1\n",
      "  (0, 4662)\t1\n",
      "  (0, 6892)\t1\n",
      "  (0, 6656)\t1\n",
      "  (0, 50)\t1\n",
      "  (0, 4743)\t1\n",
      "  (0, 4375)\t1\n",
      "  (0, 1552)\t1\n",
      "  (0, 264)\t1\n",
      "  (0, 4983)\t1\n",
      "  (0, 7424)\t1\n",
      "  (0, 3170)\t1\n",
      "  (0, 2864)\t2\n",
      "  (0, 4987)\t1\n",
      "  (0, 1572)\t1\n",
      "  (0, 3880)\t1\n",
      "  (0, 5479)\t1\n",
      "  (0, 3971)\t1\n",
      "  (0, 4781)\t1\n",
      "  (0, 5193)\t1\n",
      "  (0, 3181)\t1\n",
      "  (0, 509)\t1\n",
      "  (1, 6758)\t1\n",
      "  (1, 3316)\t1\n",
      "  :\t:\n",
      "  (4177, 3700)\t1\n",
      "  (4177, 837)\t1\n",
      "  (4177, 307)\t1\n",
      "  (4177, 6662)\t1\n",
      "  (4177, 6034)\t1\n",
      "  (4177, 4508)\t1\n",
      "  (4177, 2556)\t1\n",
      "  (4177, 5490)\t1\n",
      "  (4177, 254)\t1\n",
      "  (4177, 2744)\t1\n",
      "  (4177, 4778)\t1\n",
      "  (4177, 4446)\t1\n",
      "  (4177, 4255)\t1\n",
      "  (4177, 3629)\t1\n",
      "  (4177, 7257)\t1\n",
      "  (4177, 1574)\t1\n",
      "  (4177, 6887)\t1\n",
      "  (4177, 3738)\t1\n",
      "  (4177, 5656)\t1\n",
      "  (4177, 6514)\t1\n",
      "  (4177, 6656)\t1\n",
      "  (4178, 5999)\t1\n",
      "  (4178, 7257)\t1\n",
      "  (4178, 4238)\t1\n",
      "  (4178, 1691)\t1\n",
      "  (0, 1538)\t1\n",
      "  (0, 5189)\t1\n",
      "  (0, 6542)\t1\n",
      "  (0, 7405)\t1\n",
      "  (1, 1016)\t1\n",
      "  (1, 3050)\t1\n",
      "  (1, 4163)\t1\n",
      "  (1, 4238)\t1\n",
      "  (1, 4370)\t1\n",
      "  (1, 5200)\t1\n",
      "  (1, 6656)\t1\n",
      "  (1, 7407)\t1\n",
      "  (1, 7420)\t1\n",
      "  (2, 986)\t1\n",
      "  (2, 3244)\t1\n",
      "  (2, 7162)\t1\n",
      "  (3, 3237)\t1\n",
      "  (4, 887)\t2\n",
      "  (4, 1060)\t1\n",
      "  (4, 1595)\t1\n",
      "  (4, 2066)\t1\n",
      "  (4, 2833)\t1\n",
      "  (4, 3388)\t1\n",
      "  (4, 3623)\t1\n",
      "  (4, 3921)\t1\n",
      "  :\t:\n",
      "  (1391, 4373)\t1\n",
      "  (1391, 4413)\t1\n",
      "  (1391, 4441)\t1\n",
      "  (1391, 4743)\t1\n",
      "  (1391, 4778)\t1\n",
      "  (1391, 6017)\t1\n",
      "  (1391, 6057)\t1\n",
      "  (1391, 6829)\t1\n",
      "  (1391, 6904)\t1\n",
      "  (1391, 7012)\t1\n",
      "  (1391, 7120)\t1\n",
      "  (1391, 7230)\t2\n",
      "  (1391, 7239)\t1\n",
      "  (1391, 7287)\t1\n",
      "  (1391, 7357)\t1\n",
      "  (1392, 848)\t1\n",
      "  (1392, 2400)\t1\n",
      "  (1392, 2873)\t1\n",
      "  (1392, 3158)\t1\n",
      "  (1392, 4238)\t1\n",
      "  (1392, 4255)\t2\n",
      "  (1392, 4487)\t1\n",
      "  (1392, 4802)\t1\n",
      "  (1392, 5565)\t1\n",
      "  (1392, 7075)\t1\n"
     ]
    }
   ],
   "source": [
    "print(training_data)\n",
    "print(testing_data)\n",
    "\n",
    "# Notes:\n",
    "# Both data are in the forms of mtx loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The following code is provided (skeleton code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of getting a positive test result P(Pos) is: {} 0.10799999999999998\n"
     ]
    }
   ],
   "source": [
    "# P(D)\n",
    "p_diabetes = 0.01\n",
    "\n",
    "# P(~D)\n",
    "p_no_diabetes = 0.99\n",
    "\n",
    "# Sensitivity or P(Pos|D)\n",
    "p_pos_diabetes = 0.9\n",
    "\n",
    "# Specificity or P(Neg/~D)\n",
    "p_neg_no_diabetes = 0.9\n",
    "\n",
    "# P(Pos)\n",
    "p_pos = (p_diabetes * p_pos_diabetes) + (p_no_diabetes * (1 - p_neg_no_diabetes))\n",
    "print('The probability of getting a positive test result P(Pos) is: {}',format(p_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the chance of someone getting a positive is:  \n",
    "[[chance they have diabetes] \\* [the chance that the test will turn positive for diabetes (given diabetes)]] **+** [[the chance that they don't have diabetes] \\* [the chance of a false positive]  ]\n",
    ">**Instructions**:\n",
    "Compute the probability of an individual having diabetes, given that, that individual got a positive test result.\n",
    "In other words, compute P(D|Pos).\n",
    "\n",
    ">The formula is: P(D|Pos) = (P(D) * P(Pos|D) / P(Pos)\n",
    "\n",
    ">**Instructions**:\n",
    "Compute the probability of an individual not having diabetes, given that, that individual got a positive test result.\n",
    "In other words, compute P(~D|Pos).\n",
    "\n",
    ">The formula is: P(~D|Pos) = (P(~D) * P(Pos|~D) / P(Pos)\n",
    "\n",
    ">Note that P(Pos/~D) can be computed as 1 - P(Neg/~D). \n",
    "\n",
    ">Therefore:\n",
    "P(Pos/~D) = p_pos_no_diabetes = 1 - 0.9 = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of having diabetes if test results positive:  0.08333333333333336\n",
      "The probability of having no diabetes if test results positive:  0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "p_diabetes_pos = p_diabetes * p_pos_diabetes / p_pos \n",
    "print('The probability of having diabetes if test results positive: ', format(p_diabetes_pos))\n",
    "\n",
    "p_no_diabetes_pos = p_no_diabetes * (1 - p_neg_no_diabetes) / p_pos\n",
    "print('The probability of having no diabetes if test results positive: ', format(p_no_diabetes_pos))\n",
    "# P(pos|~D) = 1 - P(neg|~D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_no_diabetes_pos + p_diabetes_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    ">**Instructions**: Compute the probability of the words 'freedom' and 'immigration' being said in a speech, or\n",
    "P(F,I).\n",
    "\n",
    ">The first step is multiplying the probabilities of Jill Stein giving a speech with her individual \n",
    "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_j_text\n",
    "\n",
    ">The second step is multiplying the probabilities of Gary Johnson giving a speech with his individual \n",
    "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_g_text\n",
    "\n",
    ">The third step is to add both of these probabilities and you will get P(F,I)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probablity of words 'freedom' and 'immigration' being said in speech:  0.075\n"
     ]
    }
   ],
   "source": [
    "p_j, p_g = [0.5, 0.5]\n",
    "p_f_j, p_i_j, p_e_j = [0.1, 0.1, 0.8]\n",
    "p_f_g, p_i_g, p_e_g = [0.7, 0.2, 0.1]\n",
    "\n",
    "p_j_text = p_j * p_f_j * p_i_j\n",
    "p_g_text = p_g * p_f_g * p_i_g\n",
    "\n",
    "p_f_i = p_j_text + p_g_text\n",
    "\n",
    "print(\"Probablity of words 'freedom' and 'immigration' being said in speech: \", format(p_f_i) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Jill Stein saying 'freedom' and 'immigration':  0.06666666666666668\n"
     ]
    }
   ],
   "source": [
    "p_j_fi = p_j_text / p_f_i\n",
    "print(\"Probability of Jill Stein saying 'freedom' and 'immigration': \", format(p_j_fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Gary Johnson saying 'freedom' and 'immigration':  0.9333333333333332\n"
     ]
    }
   ],
   "source": [
    "p_g_fi = p_g_text / p_f_i\n",
    "print(\"Probability of Gary Johnson saying 'freedom' and 'immigration': \", format(p_g_fi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    ">**Instructions**:\n",
    "\n",
    ">We have loaded the training data into the variable 'training_data' and the testing data into the \n",
    "variable 'testing_data'.\n",
    "\n",
    ">Import the MultinomialNB classifier and fit the training data into the classifier using fit(). \n",
    "Name your classifier 'naive_bayes'. You will be training the classifier using 'training_data' and y_train' from our split earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Instructions**:\n",
    "Now that our algorithm has been trained using the training data set we can now make some predictions on the test data\n",
    "stored in '`testing_data`' using `predict()`. Save your predictions into the 'predictions' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Instructions**:\n",
    "Compute the accuracy, precision, recall and F1 scores of your model using your test data '`y_test`' and the predictions\n",
    "you made earlier stored in the '`predictions`' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy   :  0.9885139985642498\n",
      "Precision  :  0.9720670391061452\n",
      "Recall     :  0.9405405405405406\n",
      "F1 Score   :  0.9560439560439562\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "precision = metrics.precision_score(y_test, predictions)\n",
    "recall = metrics.recall_score(y_test, predictions)\n",
    "f1 = metrics.f1_score(y_test, predictions)\n",
    "\n",
    "valList = [accuracy, precision, recall, f1]\n",
    "strList = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "iterList = zip(valList,strList)\n",
    "\n",
    "for val, s in iterList:\n",
    "    print('{:10}'.format(s), ': ', format(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
